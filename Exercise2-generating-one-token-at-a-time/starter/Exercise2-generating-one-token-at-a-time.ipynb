{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Generating one token at a time\n",
        "\n",
        "In this exercise, we will get to understand how an LLM generates text--one token at a time, using the previous tokens to predict the following ones.\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1. Load a tokenizer and a model\n",
        "\n",
        "First we load a tokenizer and a model from HuggingFace's transformers library. A tokenizer is a function that splits a string into a list of numbers that the model can understand.\n",
        "\n",
        "In this exercise, all the code will be written for you. All you need to do is follow along!"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# To load a pretrained model and a tokenizer using HuggingFace, we only need two lines of code!\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# We create a partial sentence and tokenize it.\n",
        "text = \"Udacity is the best place to learn about generative\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Show the tokens as numbers, i.e. \"input_ids\"\n",
        "inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2. Examine the tokenization\n",
        "\n",
        "Let's explore what these tokens mean!"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show how the sentence is tokenized\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def show_tokenization(inputs):\n",
        "    return pd.DataFrame(\n",
        "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
        "        columns=[\"id\", \"token\"],\n",
        "    )\n",
        "\n",
        "\n",
        "show_tokenization(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subword tokenization\n",
        "\n",
        "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
        "\n",
        "## Step 2. Calculate the probability of the next token\n",
        "\n",
        "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the probabilities for the next token for all possible choices. We show the\n",
        "# top 5 choices and the corresponding words or subwords for these tokens.\n",
        "\n",
        "import torch\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "\n",
        "def show_next_token_choices(probabilities, top_n=5):\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            (id, tokenizer.decode(id), p.item())\n",
        "            for id, p in enumerate(probabilities)\n",
        "            if p.item()\n",
        "        ],\n",
        "        columns=[\"id\", \"token\", \"p\"],\n",
        "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
        "\n",
        "\n",
        "show_next_token_choices(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interesting! The model thinks that the most likely next word is \"programming\", followed up closely by \"learning\"."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtain the token id for the most probable next token\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "\n",
        "print(f\"Next token id: {next_token_id}\")\n",
        "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We append the most likely token to the text.\n",
        "text = text + tokenizer.decode(8300)\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3. Generate some more tokens\n",
        "\n",
        "The following cell will take `text`, show the most probable tokens to follow, and append the most likely token to text. Run the cell over and over to see it in action!"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Show the text\n",
        "print(text)\n",
        "\n",
        "# Convert to tokens\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Calculate the probabilities for the next token and show the top 5 choices\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "display(Markdown(\"**Next token probabilities:**\"))\n",
        "display(show_next_token_choices(probabilities))\n",
        "\n",
        "# Choose the most likely token id and add it to the text\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "text = text + tokenizer.decode(next_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4. Use the `generate` method"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Start with some text and tokenize it\n",
        "text = \"Once upon a time, generative models\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Use the `generate` method to generate lots of text\n",
        "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Show the generated text\n",
        "display(Markdown(tokenizer.decode(output[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice that GPT-2 is not nearly as sophisticated as later models like GPT-4, which you may have experience using. It often repeats itself and doesn't always make much sense. But it's still pretty impressive that it can generate text that looks like English."
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
