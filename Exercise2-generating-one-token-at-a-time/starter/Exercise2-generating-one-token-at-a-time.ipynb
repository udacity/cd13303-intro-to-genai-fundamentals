{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Generating one token at a time\n",
        "\n",
        "In this exercise, we will get to understand how an LLM generates text, one token at a time, using the previous tokens to predict the following ones. We will use the pre-trained `gpt2` model from the `transformers` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1. Load a tokenizer and a model\n",
        "\n",
        "First we load a tokenizer and a model from HuggingFace's transformers library. A tokenizer\n",
        "is a function that splits a string into a list of tokens, e.g. a sentence into a list of\n",
        "numbers. The model in this case is the GPT-2 language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"Udacity is the best place to learn about generative\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question: What do you think each token represents here? Are they words? Characters? Or something else?\n",
        "\n",
        "Spend a few minutes thinking what the correct answer might be and why. You can write your thoughts in the cell below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Write your answer here. Double-click to edit.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2. Examine the tokenization\n",
        "\n",
        "Now we will show how the tokenization actually is taking place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show how the sentence is tokenized\n",
        "import pandas as pd\n",
        "\n",
        "def show_tokenization(inputs):\n",
        "    return pd.DataFrame([(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]], columns=[\"id\", \"token\"])\n",
        "\n",
        "show_tokenization(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subword tokenization\n",
        "\n",
        "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
        "\n",
        "## Step 2. Calculate the probability of the next token\n",
        "\n",
        "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the probabilities for the next token for all possible choices. We show the\n",
        "# top 5 choices and the corresponding words or subwords for these tokens.\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits[:, -1, :]\n",
        "  probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "def show_next_token_choices(probabilities, top_n=5):\n",
        "  return pd.DataFrame(\n",
        "    [(id, tokenizer.decode(id), p.item()) for id, p in enumerate(probabilities) if p.item()],\n",
        "    columns=[\"id\", \"token\", \"p\"]\n",
        "  ).sort_values(\"p\", ascending=False)[:top_n]\n",
        "\n",
        "show_next_token_choices(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interesting! The model thinks that the most likely next word is \"programming\", but not by too much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Obtain the token id for the most probable next token\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We append the most likely token to the text. \n",
        "text = text + tokenizer.decode(8300)\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3. Generate some more tokens\n",
        "\n",
        "Now write the code you need to generate the next token. We are going to take the LLM approach to learning. That is, we will mask certain parts of the code with `<MASK>` and ask you to fill it in. Don't worry, we will give you hints along the way!\n",
        "\n",
        "Also, all of the code will come verbatim from earlier cells in this notebook. Feel free to scroll up and copy-and-paste the code you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NOW IT'S YOUR TURN: FILL IN THE PARTS LABELLED `<MASK>`\n",
        "\n",
        "# 0. We start with the text\n",
        "print(text)\n",
        "\n",
        "# 1. Convert the text to tokens\n",
        "# Hint: use the tokenizer\n",
        "inputs = <MASK>\n",
        "\n",
        "# 2. Calculate the probabilities for the next token for all possible choices.\n",
        "# Hint: A softmax converts the logits to probabilities\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits[:, -1, :]\n",
        "  probabilities = <MASK>(logits[0], dim=-1)\n",
        "\n",
        "# 3. Obtain the token id for the most probable next token.\n",
        "# Hint: argmax returns the index of the largest value\n",
        "next_token_id = <MASK>(probabilities).item()\n",
        "\n",
        "# 4. Decode the most likely token and append it to the text\n",
        "text = text + tokenizer.decode(next_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can rerun that cell many times to generate all the text you want using Ctrl+Enter. This process of starting with a string of tokens and generating the following tokens is called auto-regressive generation. After running it 30+ times, you will get an output such as the following:\n",
        "\n",
        "```\n",
        "Udacity is the best place to learn about generative programming.\n",
        "\n",
        "\n",
        "The following is a list of the top 10 most popular programming languages.\n",
        "\n",
        "\n",
        "1. C#\n",
        "\n",
        "\n",
        "2. Java\n",
        "```\n",
        "\n",
        "## Step 3. Generate the rest of the tokens\n",
        "\n",
        "We can use the `generate` method to generate the rest of the tokens in a single command. Check this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We can use the `generate` method to do this for us.\n",
        "# Play around with this and generate some more text!\n",
        "\n",
        "text = \"Generative AI\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Show the generated text\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice that GPT-2 is not nearly as sophisticated as later models like GPT-4, which you may have experience using. It often repeats itself and doesn't make much sense. But it's still pretty impressive that it can generate text that looks like English."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
